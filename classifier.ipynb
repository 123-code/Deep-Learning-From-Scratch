{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3607bd-fb87-4578-8537-b4396156752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerias necesarioas, para crear nuewtra red neuronal, transformar las imagenes a una matriz, y descargar el dataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7826616b-6582-48e6-9333-163003b45224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fae0229-4bcc-4da0-8bf7-03e58f05c052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 170498071/170498071 [00:10<00:00, 17017885.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409d90f9-50f8-4ffe-add8-5f928f904297",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb465fb-8b9e-4179-9c7b-2198d6b89604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedClasificadora(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RedClasificadora, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512) \n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8) \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d84d08b-851a-49a5-9f87-77c0f530dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RedClasificadora()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee42c74-d2f5-4b96-aa89-bebae9474e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100], Loss: 2.3027\n",
      "Epoch [1/10], Step [200], Loss: 2.3024\n",
      "Epoch [1/10], Step [300], Loss: 2.3021\n",
      "Epoch [1/10], Step [400], Loss: 2.3018\n",
      "Epoch [1/10], Step [500], Loss: 2.3012\n",
      "Epoch [1/10], Step [600], Loss: 2.3007\n",
      "Epoch [1/10], Step [700], Loss: 2.3002\n",
      "Epoch [1/10], Step [800], Loss: 2.3001\n",
      "Epoch [1/10], Step [900], Loss: 2.2997\n",
      "Epoch [1/10], Step [1000], Loss: 2.2989\n",
      "Epoch [1/10], Step [1100], Loss: 2.2980\n",
      "Epoch [1/10], Step [1200], Loss: 2.2994\n",
      "Epoch [1/10], Step [1300], Loss: 2.2979\n",
      "Epoch [1/10], Step [1400], Loss: 2.2967\n",
      "Epoch [1/10], Step [1500], Loss: 2.2957\n",
      "Epoch [1/10], Step [1600], Loss: 2.2950\n",
      "Epoch [1/10], Step [1700], Loss: 2.2935\n",
      "Epoch [1/10], Step [1800], Loss: 2.2900\n",
      "Epoch [1/10], Step [1900], Loss: 2.2850\n",
      "Epoch [1/10], Step [2000], Loss: 2.2802\n",
      "Epoch [1/10], Step [2100], Loss: 2.2805\n",
      "Epoch [1/10], Step [2200], Loss: 2.2798\n",
      "Epoch [1/10], Step [2300], Loss: 2.2624\n",
      "Epoch [1/10], Step [2400], Loss: 2.2454\n",
      "Epoch [1/10], Step [2500], Loss: 2.2591\n",
      "Epoch [1/10], Step [2600], Loss: 2.2458\n",
      "Epoch [1/10], Step [2700], Loss: 2.2569\n",
      "Epoch [1/10], Step [2800], Loss: 2.2277\n",
      "Epoch [1/10], Step [2900], Loss: 2.2180\n",
      "Epoch [1/10], Step [3000], Loss: 2.2388\n",
      "Epoch [1/10], Step [3100], Loss: 2.2244\n",
      "Epoch [1/10], Step [3200], Loss: 2.1900\n",
      "Epoch [1/10], Step [3300], Loss: 2.2203\n",
      "Epoch [1/10], Step [3400], Loss: 2.1941\n",
      "Epoch [1/10], Step [3500], Loss: 2.2081\n",
      "Epoch [1/10], Step [3600], Loss: 2.1635\n",
      "Epoch [1/10], Step [3700], Loss: 2.2257\n",
      "Epoch [1/10], Step [3800], Loss: 2.2104\n",
      "Epoch [1/10], Step [3900], Loss: 2.1999\n",
      "Epoch [1/10], Step [4000], Loss: 2.1999\n",
      "Epoch [1/10], Step [4100], Loss: 2.1819\n",
      "Epoch [1/10], Step [4200], Loss: 2.2051\n",
      "Epoch [1/10], Step [4300], Loss: 2.1413\n",
      "Epoch [1/10], Step [4400], Loss: 2.1857\n",
      "Epoch [1/10], Step [4500], Loss: 2.1449\n",
      "Epoch [1/10], Step [4600], Loss: 2.1807\n",
      "Epoch [1/10], Step [4700], Loss: 2.1817\n",
      "Epoch [1/10], Step [4800], Loss: 2.1731\n",
      "Epoch [1/10], Step [4900], Loss: 2.1640\n",
      "Epoch [1/10], Step [5000], Loss: 2.1801\n",
      "Epoch [1/10], Step [5100], Loss: 2.1540\n",
      "Epoch [1/10], Step [5200], Loss: 2.1746\n",
      "Epoch [1/10], Step [5300], Loss: 2.1702\n",
      "Epoch [1/10], Step [5400], Loss: 2.1647\n",
      "Epoch [1/10], Step [5500], Loss: 2.1375\n",
      "Epoch [1/10], Step [5600], Loss: 2.1416\n",
      "Epoch [1/10], Step [5700], Loss: 2.1814\n",
      "Epoch [1/10], Step [5800], Loss: 2.1378\n",
      "Epoch [1/10], Step [5900], Loss: 2.1356\n",
      "Epoch [1/10], Step [6000], Loss: 2.1321\n",
      "Epoch [1/10], Step [6100], Loss: 2.1650\n",
      "Epoch [1/10], Step [6200], Loss: 2.1443\n",
      "Epoch [1/10], Step [6300], Loss: 2.1587\n",
      "Epoch [1/10], Step [6400], Loss: 2.1534\n",
      "Epoch [1/10], Step [6500], Loss: 2.1642\n",
      "Epoch [1/10], Step [6600], Loss: 2.1257\n",
      "Epoch [1/10], Step [6700], Loss: 2.1396\n",
      "Epoch [1/10], Step [6800], Loss: 2.1246\n",
      "Epoch [1/10], Step [6900], Loss: 2.1163\n",
      "Epoch [1/10], Step [7000], Loss: 2.1422\n",
      "Epoch [1/10], Step [7100], Loss: 2.1220\n",
      "Epoch [1/10], Step [7200], Loss: 2.0963\n",
      "Epoch [1/10], Step [7300], Loss: 2.1134\n",
      "Epoch [1/10], Step [7400], Loss: 2.1193\n",
      "Epoch [1/10], Step [7500], Loss: 2.1141\n",
      "Epoch [1/10], Step [7600], Loss: 2.1331\n",
      "Epoch [1/10], Step [7700], Loss: 2.1036\n",
      "Epoch [1/10], Step [7800], Loss: 2.1068\n",
      "Epoch [1/10], Step [7900], Loss: 2.1446\n",
      "Epoch [1/10], Step [8000], Loss: 2.1263\n",
      "Epoch [1/10], Step [8100], Loss: 2.1221\n",
      "Epoch [1/10], Step [8200], Loss: 2.1307\n",
      "Epoch [1/10], Step [8300], Loss: 2.1513\n",
      "Epoch [1/10], Step [8400], Loss: 2.1153\n",
      "Epoch [1/10], Step [8500], Loss: 2.1216\n",
      "Epoch [1/10], Step [8600], Loss: 2.1196\n",
      "Epoch [1/10], Step [8700], Loss: 2.1145\n",
      "Epoch [1/10], Step [8800], Loss: 2.0764\n",
      "Epoch [1/10], Step [8900], Loss: 2.1222\n",
      "Epoch [1/10], Step [9000], Loss: 2.0969\n",
      "Epoch [1/10], Step [9100], Loss: 2.1020\n",
      "Epoch [1/10], Step [9200], Loss: 2.0839\n",
      "Epoch [1/10], Step [9300], Loss: 2.0978\n",
      "Epoch [1/10], Step [9400], Loss: 2.0885\n",
      "Epoch [1/10], Step [9500], Loss: 2.1131\n",
      "Epoch [1/10], Step [9600], Loss: 2.0924\n",
      "Epoch [1/10], Step [9700], Loss: 2.1159\n",
      "Epoch [1/10], Step [9800], Loss: 2.0964\n",
      "Epoch [1/10], Step [9900], Loss: 2.1070\n",
      "Epoch [1/10], Step [10000], Loss: 2.1276\n",
      "Epoch [1/10], Step [10100], Loss: 2.1100\n",
      "Epoch [1/10], Step [10200], Loss: 2.1334\n",
      "Epoch [1/10], Step [10300], Loss: 2.0481\n",
      "Epoch [1/10], Step [10400], Loss: 2.0921\n",
      "Epoch [1/10], Step [10500], Loss: 2.0691\n",
      "Epoch [1/10], Step [10600], Loss: 2.1117\n",
      "Epoch [1/10], Step [10700], Loss: 2.1104\n",
      "Epoch [1/10], Step [10800], Loss: 2.0916\n",
      "Epoch [1/10], Step [10900], Loss: 2.1121\n",
      "Epoch [1/10], Step [11000], Loss: 2.1278\n",
      "Epoch [1/10], Step [11100], Loss: 2.0830\n",
      "Epoch [1/10], Step [11200], Loss: 2.1002\n",
      "Epoch [1/10], Step [11300], Loss: 2.1031\n",
      "Epoch [1/10], Step [11400], Loss: 2.0943\n",
      "Epoch [1/10], Step [11500], Loss: 2.0929\n",
      "Epoch [1/10], Step [11600], Loss: 2.1258\n",
      "Epoch [1/10], Step [11700], Loss: 2.0728\n",
      "Epoch [1/10], Step [11800], Loss: 2.0604\n",
      "Epoch [1/10], Step [11900], Loss: 2.0620\n",
      "Epoch [1/10], Step [12000], Loss: 2.0721\n",
      "Epoch [1/10], Step [12100], Loss: 2.0602\n",
      "Epoch [1/10], Step [12200], Loss: 2.0779\n",
      "Epoch [1/10], Step [12300], Loss: 2.0646\n",
      "Epoch [1/10], Step [12400], Loss: 2.0721\n",
      "Epoch [1/10], Step [12500], Loss: 2.0759\n",
      "Epoch [2/10], Step [100], Loss: 2.0966\n",
      "Epoch [2/10], Step [200], Loss: 2.0918\n",
      "Epoch [2/10], Step [300], Loss: 2.0704\n",
      "Epoch [2/10], Step [400], Loss: 2.0466\n",
      "Epoch [2/10], Step [500], Loss: 2.0509\n",
      "Epoch [2/10], Step [600], Loss: 2.0707\n",
      "Epoch [2/10], Step [700], Loss: 2.0702\n",
      "Epoch [2/10], Step [800], Loss: 2.0558\n",
      "Epoch [2/10], Step [900], Loss: 2.0906\n",
      "Epoch [2/10], Step [1000], Loss: 2.0741\n",
      "Epoch [2/10], Step [1100], Loss: 2.0520\n",
      "Epoch [2/10], Step [1200], Loss: 2.0723\n",
      "Epoch [2/10], Step [1300], Loss: 2.0437\n",
      "Epoch [2/10], Step [1400], Loss: 2.0512\n",
      "Epoch [2/10], Step [1500], Loss: 2.0945\n",
      "Epoch [2/10], Step [1600], Loss: 2.0676\n",
      "Epoch [2/10], Step [1700], Loss: 2.0305\n",
      "Epoch [2/10], Step [1800], Loss: 2.0696\n",
      "Epoch [2/10], Step [1900], Loss: 2.0367\n",
      "Epoch [2/10], Step [2000], Loss: 2.0724\n",
      "Epoch [2/10], Step [2100], Loss: 2.0489\n",
      "Epoch [2/10], Step [2200], Loss: 2.0482\n",
      "Epoch [2/10], Step [2300], Loss: 2.0342\n",
      "Epoch [2/10], Step [2400], Loss: 2.0466\n",
      "Epoch [2/10], Step [2500], Loss: 2.0340\n",
      "Epoch [2/10], Step [2600], Loss: 2.0553\n",
      "Epoch [2/10], Step [2700], Loss: 2.0423\n",
      "Epoch [2/10], Step [2800], Loss: 2.0771\n",
      "Epoch [2/10], Step [2900], Loss: 2.0555\n",
      "Epoch [2/10], Step [3000], Loss: 2.0435\n",
      "Epoch [2/10], Step [3100], Loss: 2.0201\n",
      "Epoch [2/10], Step [3200], Loss: 2.0151\n",
      "Epoch [2/10], Step [3300], Loss: 2.0091\n",
      "Epoch [2/10], Step [3400], Loss: 2.0570\n",
      "Epoch [2/10], Step [3500], Loss: 2.0378\n",
      "Epoch [2/10], Step [3600], Loss: 2.0551\n",
      "Epoch [2/10], Step [3700], Loss: 2.0205\n",
      "Epoch [2/10], Step [3800], Loss: 2.0498\n",
      "Epoch [2/10], Step [3900], Loss: 2.0653\n",
      "Epoch [2/10], Step [4000], Loss: 2.0531\n",
      "Epoch [2/10], Step [4100], Loss: 2.0260\n",
      "Epoch [2/10], Step [4200], Loss: 2.0675\n",
      "Epoch [2/10], Step [4300], Loss: 2.0300\n",
      "Epoch [2/10], Step [4400], Loss: 2.0371\n",
      "Epoch [2/10], Step [4500], Loss: 2.0188\n",
      "Epoch [2/10], Step [4600], Loss: 2.0021\n",
      "Epoch [2/10], Step [4700], Loss: 1.9908\n",
      "Epoch [2/10], Step [4800], Loss: 2.0314\n",
      "Epoch [2/10], Step [4900], Loss: 2.0560\n",
      "Epoch [2/10], Step [5000], Loss: 2.0517\n",
      "Epoch [2/10], Step [5100], Loss: 2.0112\n",
      "Epoch [2/10], Step [5200], Loss: 2.0421\n",
      "Epoch [2/10], Step [5300], Loss: 2.0387\n",
      "Epoch [2/10], Step [5400], Loss: 2.0319\n",
      "Epoch [2/10], Step [5500], Loss: 2.0175\n",
      "Epoch [2/10], Step [5600], Loss: 2.0438\n",
      "Epoch [2/10], Step [5700], Loss: 2.0181\n",
      "Epoch [2/10], Step [5800], Loss: 2.0229\n",
      "Epoch [2/10], Step [5900], Loss: 2.0262\n",
      "Epoch [2/10], Step [6000], Loss: 2.0227\n",
      "Epoch [2/10], Step [6100], Loss: 2.0275\n",
      "Epoch [2/10], Step [6200], Loss: 2.0406\n",
      "Epoch [2/10], Step [6300], Loss: 2.0181\n",
      "Epoch [2/10], Step [6400], Loss: 2.0053\n",
      "Epoch [2/10], Step [6500], Loss: 2.0383\n",
      "Epoch [2/10], Step [6600], Loss: 2.0468\n",
      "Epoch [2/10], Step [6700], Loss: 2.0586\n",
      "Epoch [2/10], Step [6800], Loss: 2.0603\n",
      "Epoch [2/10], Step [6900], Loss: 2.0158\n",
      "Epoch [2/10], Step [7000], Loss: 2.0401\n",
      "Epoch [2/10], Step [7100], Loss: 1.9792\n",
      "Epoch [2/10], Step [7200], Loss: 2.0015\n",
      "Epoch [2/10], Step [7300], Loss: 2.0151\n",
      "Epoch [2/10], Step [7400], Loss: 2.0363\n",
      "Epoch [2/10], Step [7500], Loss: 2.0042\n",
      "Epoch [2/10], Step [7600], Loss: 2.0286\n",
      "Epoch [2/10], Step [7700], Loss: 2.0024\n",
      "Epoch [2/10], Step [7800], Loss: 2.0235\n",
      "Epoch [2/10], Step [7900], Loss: 1.9984\n",
      "Epoch [2/10], Step [8000], Loss: 1.9889\n",
      "Epoch [2/10], Step [8100], Loss: 2.0251\n",
      "Epoch [2/10], Step [8200], Loss: 2.0363\n",
      "Epoch [2/10], Step [8300], Loss: 2.0148\n",
      "Epoch [2/10], Step [8400], Loss: 1.9884\n",
      "Epoch [2/10], Step [8500], Loss: 2.0268\n",
      "Epoch [2/10], Step [8600], Loss: 2.0121\n",
      "Epoch [2/10], Step [8700], Loss: 2.0020\n",
      "Epoch [2/10], Step [8800], Loss: 2.0222\n",
      "Epoch [2/10], Step [8900], Loss: 2.0199\n",
      "Epoch [2/10], Step [9000], Loss: 1.9978\n",
      "Epoch [2/10], Step [9100], Loss: 1.9816\n",
      "Epoch [2/10], Step [9200], Loss: 2.0481\n",
      "Epoch [2/10], Step [9300], Loss: 2.0231\n",
      "Epoch [2/10], Step [9400], Loss: 2.0192\n",
      "Epoch [2/10], Step [9500], Loss: 2.0177\n",
      "Epoch [2/10], Step [9600], Loss: 2.0253\n",
      "Epoch [2/10], Step [9700], Loss: 2.0084\n",
      "Epoch [2/10], Step [9800], Loss: 2.0089\n",
      "Epoch [2/10], Step [9900], Loss: 2.0127\n",
      "Epoch [2/10], Step [10000], Loss: 1.9986\n",
      "Epoch [2/10], Step [10100], Loss: 2.0310\n",
      "Epoch [2/10], Step [10200], Loss: 2.0107\n",
      "Epoch [2/10], Step [10300], Loss: 1.9845\n",
      "Epoch [2/10], Step [10400], Loss: 2.0353\n",
      "Epoch [2/10], Step [10500], Loss: 1.9813\n",
      "Epoch [2/10], Step [10600], Loss: 2.0514\n",
      "Epoch [2/10], Step [10700], Loss: 1.9955\n",
      "Epoch [2/10], Step [10800], Loss: 1.9848\n",
      "Epoch [2/10], Step [10900], Loss: 2.0274\n",
      "Epoch [2/10], Step [11000], Loss: 1.9778\n",
      "Epoch [2/10], Step [11100], Loss: 1.9896\n",
      "Epoch [2/10], Step [11200], Loss: 2.0002\n",
      "Epoch [2/10], Step [11300], Loss: 2.0027\n",
      "Epoch [2/10], Step [11400], Loss: 1.9490\n",
      "Epoch [2/10], Step [11500], Loss: 1.9976\n",
      "Epoch [2/10], Step [11600], Loss: 2.0263\n",
      "Epoch [2/10], Step [11700], Loss: 2.0232\n",
      "Epoch [2/10], Step [11800], Loss: 2.0350\n",
      "Epoch [2/10], Step [11900], Loss: 2.0157\n",
      "Epoch [2/10], Step [12000], Loss: 2.0022\n",
      "Epoch [2/10], Step [12100], Loss: 1.9761\n",
      "Epoch [2/10], Step [12200], Loss: 1.9826\n",
      "Epoch [2/10], Step [12300], Loss: 2.0074\n",
      "Epoch [2/10], Step [12400], Loss: 1.9900\n",
      "Epoch [2/10], Step [12500], Loss: 2.0008\n",
      "Epoch [3/10], Step [100], Loss: 1.9791\n",
      "Epoch [3/10], Step [200], Loss: 1.9812\n",
      "Epoch [3/10], Step [300], Loss: 1.9985\n",
      "Epoch [3/10], Step [400], Loss: 1.9681\n",
      "Epoch [3/10], Step [500], Loss: 1.9706\n",
      "Epoch [3/10], Step [600], Loss: 1.9880\n",
      "Epoch [3/10], Step [700], Loss: 2.0220\n",
      "Epoch [3/10], Step [800], Loss: 1.9721\n",
      "Epoch [3/10], Step [900], Loss: 1.9561\n",
      "Epoch [3/10], Step [1000], Loss: 2.0017\n",
      "Epoch [3/10], Step [1100], Loss: 1.9909\n",
      "Epoch [3/10], Step [1200], Loss: 1.9757\n",
      "Epoch [3/10], Step [1300], Loss: 2.0033\n",
      "Epoch [3/10], Step [1400], Loss: 2.0014\n",
      "Epoch [3/10], Step [1500], Loss: 1.9419\n",
      "Epoch [3/10], Step [1600], Loss: 2.0051\n",
      "Epoch [3/10], Step [1700], Loss: 1.9700\n",
      "Epoch [3/10], Step [1800], Loss: 1.9711\n",
      "Epoch [3/10], Step [1900], Loss: 1.9605\n",
      "Epoch [3/10], Step [2000], Loss: 2.0126\n",
      "Epoch [3/10], Step [2100], Loss: 1.9321\n",
      "Epoch [3/10], Step [2200], Loss: 1.9755\n",
      "Epoch [3/10], Step [2300], Loss: 1.9649\n",
      "Epoch [3/10], Step [2400], Loss: 1.9823\n",
      "Epoch [3/10], Step [2500], Loss: 2.0117\n",
      "Epoch [3/10], Step [2600], Loss: 1.9924\n",
      "Epoch [3/10], Step [2700], Loss: 1.9605\n",
      "Epoch [3/10], Step [2800], Loss: 1.9732\n",
      "Epoch [3/10], Step [2900], Loss: 1.9578\n",
      "Epoch [3/10], Step [3000], Loss: 1.9804\n",
      "Epoch [3/10], Step [3100], Loss: 1.9718\n",
      "Epoch [3/10], Step [3200], Loss: 1.9781\n",
      "Epoch [3/10], Step [3300], Loss: 1.9954\n",
      "Epoch [3/10], Step [3400], Loss: 1.9892\n",
      "Epoch [3/10], Step [3500], Loss: 1.9931\n",
      "Epoch [3/10], Step [3600], Loss: 1.9595\n",
      "Epoch [3/10], Step [3700], Loss: 1.9780\n",
      "Epoch [3/10], Step [3800], Loss: 1.9937\n",
      "Epoch [3/10], Step [3900], Loss: 1.9837\n",
      "Epoch [3/10], Step [4000], Loss: 1.9508\n",
      "Epoch [3/10], Step [4100], Loss: 1.9979\n",
      "Epoch [3/10], Step [4200], Loss: 1.9446\n",
      "Epoch [3/10], Step [4300], Loss: 1.9687\n",
      "Epoch [3/10], Step [4400], Loss: 1.9838\n",
      "Epoch [3/10], Step [4500], Loss: 1.9552\n",
      "Epoch [3/10], Step [4600], Loss: 1.9425\n",
      "Epoch [3/10], Step [4700], Loss: 1.9589\n",
      "Epoch [3/10], Step [4800], Loss: 1.9720\n",
      "Epoch [3/10], Step [4900], Loss: 1.9920\n",
      "Epoch [3/10], Step [5000], Loss: 1.9279\n",
      "Epoch [3/10], Step [5100], Loss: 1.9133\n",
      "Epoch [3/10], Step [5200], Loss: 1.9734\n",
      "Epoch [3/10], Step [5300], Loss: 1.9195\n",
      "Epoch [3/10], Step [5400], Loss: 2.0058\n",
      "Epoch [3/10], Step [5500], Loss: 1.9680\n",
      "Epoch [3/10], Step [5600], Loss: 1.9389\n",
      "Epoch [3/10], Step [5700], Loss: 1.9846\n",
      "Epoch [3/10], Step [5800], Loss: 1.9506\n",
      "Epoch [3/10], Step [5900], Loss: 1.9508\n",
      "Epoch [3/10], Step [6000], Loss: 1.9998\n",
      "Epoch [3/10], Step [6100], Loss: 1.9715\n",
      "Epoch [3/10], Step [6200], Loss: 1.9594\n",
      "Epoch [3/10], Step [6300], Loss: 1.9869\n",
      "Epoch [3/10], Step [6400], Loss: 1.9628\n",
      "Epoch [3/10], Step [6500], Loss: 1.9631\n",
      "Epoch [3/10], Step [6600], Loss: 1.9846\n",
      "Epoch [3/10], Step [6700], Loss: 1.9888\n",
      "Epoch [3/10], Step [6800], Loss: 1.9593\n",
      "Epoch [3/10], Step [6900], Loss: 1.9233\n",
      "Epoch [3/10], Step [7000], Loss: 1.9905\n",
      "Epoch [3/10], Step [7100], Loss: 1.9674\n",
      "Epoch [3/10], Step [7200], Loss: 1.9911\n",
      "Epoch [3/10], Step [7300], Loss: 1.9956\n",
      "Epoch [3/10], Step [7400], Loss: 1.9575\n",
      "Epoch [3/10], Step [7500], Loss: 1.9287\n",
      "Epoch [3/10], Step [7600], Loss: 1.9742\n",
      "Epoch [3/10], Step [7700], Loss: 1.9243\n",
      "Epoch [3/10], Step [7800], Loss: 1.9398\n",
      "Epoch [3/10], Step [7900], Loss: 1.9869\n",
      "Epoch [3/10], Step [8000], Loss: 1.9764\n",
      "Epoch [3/10], Step [8100], Loss: 1.9498\n",
      "Epoch [3/10], Step [8200], Loss: 1.9590\n",
      "Epoch [3/10], Step [8300], Loss: 1.9464\n",
      "Epoch [3/10], Step [8400], Loss: 1.9925\n",
      "Epoch [3/10], Step [8500], Loss: 1.9872\n",
      "Epoch [3/10], Step [8600], Loss: 1.9322\n",
      "Epoch [3/10], Step [8700], Loss: 1.9584\n",
      "Epoch [3/10], Step [8800], Loss: 2.0052\n",
      "Epoch [3/10], Step [8900], Loss: 1.9696\n",
      "Epoch [3/10], Step [9000], Loss: 1.9536\n",
      "Epoch [3/10], Step [9100], Loss: 1.9579\n",
      "Epoch [3/10], Step [9200], Loss: 1.9507\n",
      "Epoch [3/10], Step [9300], Loss: 1.9502\n",
      "Epoch [3/10], Step [9400], Loss: 1.9468\n",
      "Epoch [3/10], Step [9500], Loss: 1.9453\n",
      "Epoch [3/10], Step [9600], Loss: 1.9348\n",
      "Epoch [3/10], Step [9700], Loss: 1.9761\n",
      "Epoch [3/10], Step [9800], Loss: 1.9415\n",
      "Epoch [3/10], Step [9900], Loss: 1.9627\n",
      "Epoch [3/10], Step [10000], Loss: 1.9228\n",
      "Epoch [3/10], Step [10100], Loss: 1.9665\n",
      "Epoch [3/10], Step [10200], Loss: 1.9630\n",
      "Epoch [3/10], Step [10300], Loss: 1.9631\n",
      "Epoch [3/10], Step [10400], Loss: 1.9509\n",
      "Epoch [3/10], Step [10500], Loss: 1.9054\n",
      "Epoch [3/10], Step [10600], Loss: 1.9795\n",
      "Epoch [3/10], Step [10700], Loss: 1.9365\n",
      "Epoch [3/10], Step [10800], Loss: 1.9311\n",
      "Epoch [3/10], Step [10900], Loss: 1.8629\n",
      "Epoch [3/10], Step [11000], Loss: 1.9935\n",
      "Epoch [3/10], Step [11100], Loss: 1.9643\n",
      "Epoch [3/10], Step [11200], Loss: 1.9813\n",
      "Epoch [3/10], Step [11300], Loss: 1.9041\n",
      "Epoch [3/10], Step [11400], Loss: 1.9193\n",
      "Epoch [3/10], Step [11500], Loss: 1.9315\n",
      "Epoch [3/10], Step [11600], Loss: 1.9537\n",
      "Epoch [3/10], Step [11700], Loss: 1.9596\n",
      "Epoch [3/10], Step [11800], Loss: 1.9331\n",
      "Epoch [3/10], Step [11900], Loss: 1.9535\n",
      "Epoch [3/10], Step [12000], Loss: 1.9599\n",
      "Epoch [3/10], Step [12100], Loss: 1.9461\n",
      "Epoch [3/10], Step [12200], Loss: 1.9289\n",
      "Epoch [3/10], Step [12300], Loss: 1.9516\n",
      "Epoch [3/10], Step [12400], Loss: 1.9387\n",
      "Epoch [3/10], Step [12500], Loss: 1.9666\n",
      "Epoch [4/10], Step [100], Loss: 1.9128\n",
      "Epoch [4/10], Step [200], Loss: 1.9857\n",
      "Epoch [4/10], Step [300], Loss: 1.8997\n",
      "Epoch [4/10], Step [400], Loss: 1.9265\n",
      "Epoch [4/10], Step [500], Loss: 1.9055\n",
      "Epoch [4/10], Step [600], Loss: 1.9414\n",
      "Epoch [4/10], Step [700], Loss: 1.9198\n",
      "Epoch [4/10], Step [800], Loss: 1.8984\n",
      "Epoch [4/10], Step [900], Loss: 1.8998\n",
      "Epoch [4/10], Step [1000], Loss: 1.9167\n",
      "Epoch [4/10], Step [1100], Loss: 1.9539\n",
      "Epoch [4/10], Step [1200], Loss: 1.9166\n",
      "Epoch [4/10], Step [1300], Loss: 1.9149\n",
      "Epoch [4/10], Step [1400], Loss: 1.9330\n",
      "Epoch [4/10], Step [1500], Loss: 1.9184\n",
      "Epoch [4/10], Step [1600], Loss: 1.9420\n",
      "Epoch [4/10], Step [1700], Loss: 1.9459\n",
      "Epoch [4/10], Step [1800], Loss: 1.9307\n",
      "Epoch [4/10], Step [1900], Loss: 1.9193\n",
      "Epoch [4/10], Step [2000], Loss: 1.9358\n",
      "Epoch [4/10], Step [2100], Loss: 1.8987\n",
      "Epoch [4/10], Step [2200], Loss: 1.9529\n",
      "Epoch [4/10], Step [2300], Loss: 1.9827\n",
      "Epoch [4/10], Step [2400], Loss: 1.8859\n",
      "Epoch [4/10], Step [2500], Loss: 1.9274\n",
      "Epoch [4/10], Step [2600], Loss: 1.9149\n",
      "Epoch [4/10], Step [2700], Loss: 1.9420\n",
      "Epoch [4/10], Step [2800], Loss: 1.9460\n",
      "Epoch [4/10], Step [2900], Loss: 1.9201\n",
      "Epoch [4/10], Step [3000], Loss: 1.9250\n",
      "Epoch [4/10], Step [3100], Loss: 1.9171\n",
      "Epoch [4/10], Step [3200], Loss: 1.9580\n",
      "Epoch [4/10], Step [3300], Loss: 1.9177\n",
      "Epoch [4/10], Step [3400], Loss: 1.9285\n",
      "Epoch [4/10], Step [3500], Loss: 1.9195\n",
      "Epoch [4/10], Step [3600], Loss: 1.9211\n",
      "Epoch [4/10], Step [3700], Loss: 1.9093\n",
      "Epoch [4/10], Step [3800], Loss: 1.9151\n",
      "Epoch [4/10], Step [3900], Loss: 1.8998\n",
      "Epoch [4/10], Step [4000], Loss: 1.9419\n",
      "Epoch [4/10], Step [4100], Loss: 1.9231\n",
      "Epoch [4/10], Step [4200], Loss: 1.8850\n",
      "Epoch [4/10], Step [4300], Loss: 1.9528\n",
      "Epoch [4/10], Step [4400], Loss: 1.9437\n",
      "Epoch [4/10], Step [4500], Loss: 1.9720\n",
      "Epoch [4/10], Step [4600], Loss: 1.9356\n",
      "Epoch [4/10], Step [4700], Loss: 1.8917\n",
      "Epoch [4/10], Step [4800], Loss: 1.9300\n",
      "Epoch [4/10], Step [4900], Loss: 1.9279\n",
      "Epoch [4/10], Step [5000], Loss: 1.9160\n",
      "Epoch [4/10], Step [5100], Loss: 1.9209\n",
      "Epoch [4/10], Step [5200], Loss: 1.8846\n",
      "Epoch [4/10], Step [5300], Loss: 1.9261\n",
      "Epoch [4/10], Step [5400], Loss: 1.9140\n",
      "Epoch [4/10], Step [5500], Loss: 1.9080\n",
      "Epoch [4/10], Step [5600], Loss: 1.9191\n",
      "Epoch [4/10], Step [5700], Loss: 1.8794\n",
      "Epoch [4/10], Step [5800], Loss: 1.9529\n",
      "Epoch [4/10], Step [5900], Loss: 1.8951\n",
      "Epoch [4/10], Step [6000], Loss: 1.9173\n",
      "Epoch [4/10], Step [6100], Loss: 1.9284\n",
      "Epoch [4/10], Step [6200], Loss: 1.9374\n",
      "Epoch [4/10], Step [6300], Loss: 1.9238\n",
      "Epoch [4/10], Step [6400], Loss: 1.8996\n",
      "Epoch [4/10], Step [6500], Loss: 1.9072\n",
      "Epoch [4/10], Step [6600], Loss: 1.9101\n",
      "Epoch [4/10], Step [6700], Loss: 1.9535\n",
      "Epoch [4/10], Step [6800], Loss: 1.8976\n",
      "Epoch [4/10], Step [6900], Loss: 1.8815\n",
      "Epoch [4/10], Step [7000], Loss: 1.9348\n",
      "Epoch [4/10], Step [7100], Loss: 1.8918\n",
      "Epoch [4/10], Step [7200], Loss: 1.8780\n",
      "Epoch [4/10], Step [7300], Loss: 1.9051\n",
      "Epoch [4/10], Step [7400], Loss: 1.9114\n",
      "Epoch [4/10], Step [7500], Loss: 1.9334\n",
      "Epoch [4/10], Step [7600], Loss: 1.9542\n",
      "Epoch [4/10], Step [7700], Loss: 1.9028\n",
      "Epoch [4/10], Step [7800], Loss: 1.8877\n",
      "Epoch [4/10], Step [7900], Loss: 1.9061\n",
      "Epoch [4/10], Step [8000], Loss: 1.8994\n",
      "Epoch [4/10], Step [8100], Loss: 1.8989\n",
      "Epoch [4/10], Step [8200], Loss: 1.9221\n",
      "Epoch [4/10], Step [8300], Loss: 1.9127\n",
      "Epoch [4/10], Step [8400], Loss: 1.9381\n",
      "Epoch [4/10], Step [8500], Loss: 1.9496\n",
      "Epoch [4/10], Step [8600], Loss: 1.8745\n",
      "Epoch [4/10], Step [8700], Loss: 1.9033\n",
      "Epoch [4/10], Step [8800], Loss: 1.8811\n",
      "Epoch [4/10], Step [8900], Loss: 1.8924\n",
      "Epoch [4/10], Step [9000], Loss: 1.9434\n",
      "Epoch [4/10], Step [9100], Loss: 1.9042\n",
      "Epoch [4/10], Step [9200], Loss: 1.8952\n",
      "Epoch [4/10], Step [9300], Loss: 1.9176\n",
      "Epoch [4/10], Step [9400], Loss: 1.9214\n",
      "Epoch [4/10], Step [9500], Loss: 1.9017\n",
      "Epoch [4/10], Step [9600], Loss: 1.9340\n",
      "Epoch [4/10], Step [9700], Loss: 1.8841\n",
      "Epoch [4/10], Step [9800], Loss: 1.9426\n",
      "Epoch [4/10], Step [9900], Loss: 1.8774\n",
      "Epoch [4/10], Step [10000], Loss: 1.9567\n",
      "Epoch [4/10], Step [10100], Loss: 1.8935\n",
      "Epoch [4/10], Step [10200], Loss: 1.9478\n",
      "Epoch [4/10], Step [10300], Loss: 1.9181\n",
      "Epoch [4/10], Step [10400], Loss: 1.8941\n",
      "Epoch [4/10], Step [10500], Loss: 1.9248\n",
      "Epoch [4/10], Step [10600], Loss: 1.9031\n",
      "Epoch [4/10], Step [10700], Loss: 1.9200\n",
      "Epoch [4/10], Step [10800], Loss: 1.9079\n",
      "Epoch [4/10], Step [10900], Loss: 1.9068\n",
      "Epoch [4/10], Step [11000], Loss: 1.9008\n",
      "Epoch [4/10], Step [11100], Loss: 1.8965\n",
      "Epoch [4/10], Step [11200], Loss: 1.9140\n",
      "Epoch [4/10], Step [11300], Loss: 1.8819\n",
      "Epoch [4/10], Step [11400], Loss: 1.8624\n",
      "Epoch [4/10], Step [11500], Loss: 1.9379\n",
      "Epoch [4/10], Step [11600], Loss: 1.9212\n",
      "Epoch [4/10], Step [11700], Loss: 1.9278\n",
      "Epoch [4/10], Step [11800], Loss: 1.9426\n",
      "Epoch [4/10], Step [11900], Loss: 1.9204\n",
      "Epoch [4/10], Step [12000], Loss: 1.9282\n",
      "Epoch [4/10], Step [12100], Loss: 1.8889\n",
      "Epoch [4/10], Step [12200], Loss: 1.9032\n",
      "Epoch [4/10], Step [12300], Loss: 1.9173\n",
      "Epoch [4/10], Step [12400], Loss: 1.9176\n",
      "Epoch [4/10], Step [12500], Loss: 1.9109\n",
      "Epoch [5/10], Step [100], Loss: 1.9199\n",
      "Epoch [5/10], Step [200], Loss: 1.8606\n",
      "Epoch [5/10], Step [300], Loss: 1.9168\n",
      "Epoch [5/10], Step [400], Loss: 1.9018\n",
      "Epoch [5/10], Step [500], Loss: 1.9014\n",
      "Epoch [5/10], Step [600], Loss: 1.8904\n",
      "Epoch [5/10], Step [700], Loss: 1.9448\n",
      "Epoch [5/10], Step [800], Loss: 1.8851\n",
      "Epoch [5/10], Step [900], Loss: 1.9227\n",
      "Epoch [5/10], Step [1000], Loss: 1.8540\n",
      "Epoch [5/10], Step [1100], Loss: 1.9079\n",
      "Epoch [5/10], Step [1200], Loss: 1.8604\n",
      "Epoch [5/10], Step [1300], Loss: 1.8737\n",
      "Epoch [5/10], Step [1400], Loss: 1.8875\n",
      "Epoch [5/10], Step [1500], Loss: 1.9058\n",
      "Epoch [5/10], Step [1600], Loss: 1.8555\n",
      "Epoch [5/10], Step [1700], Loss: 1.8745\n",
      "Epoch [5/10], Step [1800], Loss: 1.8872\n",
      "Epoch [5/10], Step [1900], Loss: 1.8785\n",
      "Epoch [5/10], Step [2000], Loss: 1.8711\n",
      "Epoch [5/10], Step [2100], Loss: 1.8987\n",
      "Epoch [5/10], Step [2200], Loss: 1.8425\n",
      "Epoch [5/10], Step [2300], Loss: 1.9096\n",
      "Epoch [5/10], Step [2400], Loss: 1.8888\n",
      "Epoch [5/10], Step [2500], Loss: 1.9017\n",
      "Epoch [5/10], Step [2600], Loss: 1.8503\n",
      "Epoch [5/10], Step [2700], Loss: 1.8751\n",
      "Epoch [5/10], Step [2800], Loss: 1.8631\n",
      "Epoch [5/10], Step [2900], Loss: 1.8836\n",
      "Epoch [5/10], Step [3000], Loss: 1.8737\n",
      "Epoch [5/10], Step [3100], Loss: 1.8461\n",
      "Epoch [5/10], Step [3200], Loss: 1.9165\n",
      "Epoch [5/10], Step [3300], Loss: 1.8455\n",
      "Epoch [5/10], Step [3400], Loss: 1.8920\n",
      "Epoch [5/10], Step [3500], Loss: 1.8586\n",
      "Epoch [5/10], Step [3600], Loss: 1.9521\n",
      "Epoch [5/10], Step [3700], Loss: 1.9191\n",
      "Epoch [5/10], Step [3800], Loss: 1.8339\n",
      "Epoch [5/10], Step [3900], Loss: 1.8611\n",
      "Epoch [5/10], Step [4000], Loss: 1.8912\n",
      "Epoch [5/10], Step [4100], Loss: 1.8669\n",
      "Epoch [5/10], Step [4200], Loss: 1.9007\n",
      "Epoch [5/10], Step [4300], Loss: 1.8788\n",
      "Epoch [5/10], Step [4400], Loss: 1.8760\n",
      "Epoch [5/10], Step [4500], Loss: 1.8599\n",
      "Epoch [5/10], Step [4600], Loss: 1.8974\n",
      "Epoch [5/10], Step [4700], Loss: 1.8752\n",
      "Epoch [5/10], Step [4800], Loss: 1.9023\n",
      "Epoch [5/10], Step [4900], Loss: 1.8868\n",
      "Epoch [5/10], Step [5000], Loss: 1.8984\n",
      "Epoch [5/10], Step [5100], Loss: 1.8816\n",
      "Epoch [5/10], Step [5200], Loss: 1.9079\n",
      "Epoch [5/10], Step [5300], Loss: 1.8866\n",
      "Epoch [5/10], Step [5400], Loss: 1.8667\n",
      "Epoch [5/10], Step [5500], Loss: 1.9152\n",
      "Epoch [5/10], Step [5600], Loss: 1.9133\n",
      "Epoch [5/10], Step [5700], Loss: 1.9074\n",
      "Epoch [5/10], Step [5800], Loss: 1.8729\n",
      "Epoch [5/10], Step [5900], Loss: 1.8828\n",
      "Epoch [5/10], Step [6000], Loss: 1.9152\n",
      "Epoch [5/10], Step [6100], Loss: 1.8786\n",
      "Epoch [5/10], Step [6200], Loss: 1.8389\n",
      "Epoch [5/10], Step [6300], Loss: 1.8806\n",
      "Epoch [5/10], Step [6400], Loss: 1.8947\n",
      "Epoch [5/10], Step [6500], Loss: 1.8618\n",
      "Epoch [5/10], Step [6600], Loss: 1.8558\n",
      "Epoch [5/10], Step [6700], Loss: 1.9081\n",
      "Epoch [5/10], Step [6800], Loss: 1.8937\n",
      "Epoch [5/10], Step [6900], Loss: 1.9170\n",
      "Epoch [5/10], Step [7000], Loss: 1.8919\n",
      "Epoch [5/10], Step [7100], Loss: 1.8669\n",
      "Epoch [5/10], Step [7200], Loss: 1.8941\n",
      "Epoch [5/10], Step [7300], Loss: 1.9108\n",
      "Epoch [5/10], Step [7400], Loss: 1.8733\n",
      "Epoch [5/10], Step [7500], Loss: 1.8961\n",
      "Epoch [5/10], Step [7600], Loss: 1.8640\n",
      "Epoch [5/10], Step [7700], Loss: 1.8912\n",
      "Epoch [5/10], Step [7800], Loss: 1.8436\n",
      "Epoch [5/10], Step [7900], Loss: 1.8851\n",
      "Epoch [5/10], Step [8000], Loss: 1.9316\n",
      "Epoch [5/10], Step [8100], Loss: 1.8501\n",
      "Epoch [5/10], Step [8200], Loss: 1.8964\n",
      "Epoch [5/10], Step [8300], Loss: 1.8336\n",
      "Epoch [5/10], Step [8400], Loss: 1.8633\n",
      "Epoch [5/10], Step [8500], Loss: 1.9255\n",
      "Epoch [5/10], Step [8600], Loss: 1.9171\n",
      "Epoch [5/10], Step [8700], Loss: 1.8671\n",
      "Epoch [5/10], Step [8800], Loss: 1.8992\n",
      "Epoch [5/10], Step [8900], Loss: 1.8380\n",
      "Epoch [5/10], Step [9000], Loss: 1.8636\n",
      "Epoch [5/10], Step [9100], Loss: 1.8880\n",
      "Epoch [5/10], Step [9200], Loss: 1.8546\n",
      "Epoch [5/10], Step [9300], Loss: 1.8763\n",
      "Epoch [5/10], Step [9400], Loss: 1.8801\n",
      "Epoch [5/10], Step [9500], Loss: 1.8319\n",
      "Epoch [5/10], Step [9600], Loss: 1.8829\n",
      "Epoch [5/10], Step [9700], Loss: 1.8836\n",
      "Epoch [5/10], Step [9800], Loss: 1.9051\n",
      "Epoch [5/10], Step [9900], Loss: 1.8724\n",
      "Epoch [5/10], Step [10000], Loss: 1.8764\n",
      "Epoch [5/10], Step [10100], Loss: 1.8481\n",
      "Epoch [5/10], Step [10200], Loss: 1.8392\n",
      "Epoch [5/10], Step [10300], Loss: 1.9045\n",
      "Epoch [5/10], Step [10400], Loss: 1.8593\n",
      "Epoch [5/10], Step [10500], Loss: 1.8655\n",
      "Epoch [5/10], Step [10600], Loss: 1.9270\n",
      "Epoch [5/10], Step [10700], Loss: 1.8900\n",
      "Epoch [5/10], Step [10800], Loss: 1.8878\n",
      "Epoch [5/10], Step [10900], Loss: 1.8517\n",
      "Epoch [5/10], Step [11000], Loss: 1.8708\n",
      "Epoch [5/10], Step [11100], Loss: 1.8838\n",
      "Epoch [5/10], Step [11200], Loss: 1.8945\n",
      "Epoch [5/10], Step [11300], Loss: 1.8806\n",
      "Epoch [5/10], Step [11400], Loss: 1.8343\n",
      "Epoch [5/10], Step [11500], Loss: 1.8278\n",
      "Epoch [5/10], Step [11600], Loss: 1.8119\n",
      "Epoch [5/10], Step [11700], Loss: 1.8623\n",
      "Epoch [5/10], Step [11800], Loss: 1.8758\n",
      "Epoch [5/10], Step [11900], Loss: 1.8697\n",
      "Epoch [5/10], Step [12000], Loss: 1.8646\n",
      "Epoch [5/10], Step [12100], Loss: 1.8654\n",
      "Epoch [5/10], Step [12200], Loss: 1.8913\n",
      "Epoch [5/10], Step [12300], Loss: 1.8707\n",
      "Epoch [5/10], Step [12400], Loss: 1.9071\n",
      "Epoch [5/10], Step [12500], Loss: 1.8873\n",
      "Epoch [6/10], Step [100], Loss: 1.8392\n",
      "Epoch [6/10], Step [200], Loss: 1.8750\n",
      "Epoch [6/10], Step [300], Loss: 1.8684\n",
      "Epoch [6/10], Step [400], Loss: 1.8997\n",
      "Epoch [6/10], Step [500], Loss: 1.8547\n",
      "Epoch [6/10], Step [600], Loss: 1.8477\n",
      "Epoch [6/10], Step [700], Loss: 1.8546\n",
      "Epoch [6/10], Step [800], Loss: 1.8500\n",
      "Epoch [6/10], Step [900], Loss: 1.7954\n",
      "Epoch [6/10], Step [1000], Loss: 1.8769\n",
      "Epoch [6/10], Step [1100], Loss: 1.8557\n",
      "Epoch [6/10], Step [1200], Loss: 1.8603\n",
      "Epoch [6/10], Step [1300], Loss: 1.8464\n",
      "Epoch [6/10], Step [1400], Loss: 1.8534\n",
      "Epoch [6/10], Step [1500], Loss: 1.8385\n",
      "Epoch [6/10], Step [1600], Loss: 1.8375\n",
      "Epoch [6/10], Step [1700], Loss: 1.8337\n",
      "Epoch [6/10], Step [1800], Loss: 1.8542\n",
      "Epoch [6/10], Step [1900], Loss: 1.8511\n",
      "Epoch [6/10], Step [2000], Loss: 1.8424\n",
      "Epoch [6/10], Step [2100], Loss: 1.8188\n",
      "Epoch [6/10], Step [2200], Loss: 1.8673\n",
      "Epoch [6/10], Step [2300], Loss: 1.8451\n",
      "Epoch [6/10], Step [2400], Loss: 1.8595\n",
      "Epoch [6/10], Step [2500], Loss: 1.8240\n",
      "Epoch [6/10], Step [2600], Loss: 1.8660\n",
      "Epoch [6/10], Step [2700], Loss: 1.8462\n",
      "Epoch [6/10], Step [2800], Loss: 1.8360\n",
      "Epoch [6/10], Step [2900], Loss: 1.8854\n",
      "Epoch [6/10], Step [3000], Loss: 1.8149\n",
      "Epoch [6/10], Step [3100], Loss: 1.8481\n",
      "Epoch [6/10], Step [3200], Loss: 1.8229\n",
      "Epoch [6/10], Step [3300], Loss: 1.8639\n",
      "Epoch [6/10], Step [3400], Loss: 1.8140\n",
      "Epoch [6/10], Step [3500], Loss: 1.8513\n",
      "Epoch [6/10], Step [3600], Loss: 1.8653\n",
      "Epoch [6/10], Step [3700], Loss: 1.8731\n",
      "Epoch [6/10], Step [3800], Loss: 1.8722\n",
      "Epoch [6/10], Step [3900], Loss: 1.8270\n",
      "Epoch [6/10], Step [4000], Loss: 1.8419\n",
      "Epoch [6/10], Step [4100], Loss: 1.8407\n",
      "Epoch [6/10], Step [4200], Loss: 1.8753\n",
      "Epoch [6/10], Step [4300], Loss: 1.8714\n",
      "Epoch [6/10], Step [4400], Loss: 1.9034\n",
      "Epoch [6/10], Step [4500], Loss: 1.8489\n",
      "Epoch [6/10], Step [4600], Loss: 1.8739\n",
      "Epoch [6/10], Step [4700], Loss: 1.8073\n",
      "Epoch [6/10], Step [4800], Loss: 1.8516\n",
      "Epoch [6/10], Step [4900], Loss: 1.8492\n",
      "Epoch [6/10], Step [5000], Loss: 1.8299\n",
      "Epoch [6/10], Step [5100], Loss: 1.7997\n",
      "Epoch [6/10], Step [5200], Loss: 1.8395\n",
      "Epoch [6/10], Step [5300], Loss: 1.8599\n",
      "Epoch [6/10], Step [5400], Loss: 1.8241\n",
      "Epoch [6/10], Step [5500], Loss: 1.8920\n",
      "Epoch [6/10], Step [5600], Loss: 1.8258\n",
      "Epoch [6/10], Step [5700], Loss: 1.8648\n",
      "Epoch [6/10], Step [5800], Loss: 1.8795\n",
      "Epoch [6/10], Step [5900], Loss: 1.9121\n",
      "Epoch [6/10], Step [6000], Loss: 1.8262\n",
      "Epoch [6/10], Step [6100], Loss: 1.8296\n",
      "Epoch [6/10], Step [6200], Loss: 1.8554\n",
      "Epoch [6/10], Step [6300], Loss: 1.8444\n",
      "Epoch [6/10], Step [6400], Loss: 1.8533\n",
      "Epoch [6/10], Step [6500], Loss: 1.8033\n",
      "Epoch [6/10], Step [6600], Loss: 1.8192\n",
      "Epoch [6/10], Step [6700], Loss: 1.8482\n",
      "Epoch [6/10], Step [6800], Loss: 1.8483\n",
      "Epoch [6/10], Step [6900], Loss: 1.8384\n",
      "Epoch [6/10], Step [7000], Loss: 1.8499\n",
      "Epoch [6/10], Step [7100], Loss: 1.8711\n",
      "Epoch [6/10], Step [7200], Loss: 1.8637\n",
      "Epoch [6/10], Step [7300], Loss: 1.8478\n",
      "Epoch [6/10], Step [7400], Loss: 1.8700\n",
      "Epoch [6/10], Step [7500], Loss: 1.8567\n",
      "Epoch [6/10], Step [7600], Loss: 1.8968\n",
      "Epoch [6/10], Step [7700], Loss: 1.8731\n",
      "Epoch [6/10], Step [7800], Loss: 1.8729\n",
      "Epoch [6/10], Step [7900], Loss: 1.8486\n",
      "Epoch [6/10], Step [8000], Loss: 1.8371\n",
      "Epoch [6/10], Step [8100], Loss: 1.8601\n",
      "Epoch [6/10], Step [8200], Loss: 1.8808\n",
      "Epoch [6/10], Step [8300], Loss: 1.8328\n",
      "Epoch [6/10], Step [8400], Loss: 1.8280\n",
      "Epoch [6/10], Step [8500], Loss: 1.8534\n",
      "Epoch [6/10], Step [8600], Loss: 1.8419\n",
      "Epoch [6/10], Step [8700], Loss: 1.8480\n",
      "Epoch [6/10], Step [8800], Loss: 1.8867\n",
      "Epoch [6/10], Step [8900], Loss: 1.8735\n",
      "Epoch [6/10], Step [9000], Loss: 1.8281\n",
      "Epoch [6/10], Step [9100], Loss: 1.8586\n",
      "Epoch [6/10], Step [9200], Loss: 1.8462\n",
      "Epoch [6/10], Step [9300], Loss: 1.8571\n",
      "Epoch [6/10], Step [9400], Loss: 1.8127\n",
      "Epoch [6/10], Step [9500], Loss: 1.8357\n",
      "Epoch [6/10], Step [9600], Loss: 1.8561\n",
      "Epoch [6/10], Step [9700], Loss: 1.8735\n",
      "Epoch [6/10], Step [9800], Loss: 1.8367\n",
      "Epoch [6/10], Step [9900], Loss: 1.8410\n",
      "Epoch [6/10], Step [10000], Loss: 1.8584\n",
      "Epoch [6/10], Step [10100], Loss: 1.8420\n",
      "Epoch [6/10], Step [10200], Loss: 1.8240\n",
      "Epoch [6/10], Step [10300], Loss: 1.8337\n",
      "Epoch [6/10], Step [10400], Loss: 1.8543\n",
      "Epoch [6/10], Step [10500], Loss: 1.8563\n",
      "Epoch [6/10], Step [10600], Loss: 1.8330\n",
      "Epoch [6/10], Step [10700], Loss: 1.8393\n",
      "Epoch [6/10], Step [10800], Loss: 1.8575\n",
      "Epoch [6/10], Step [10900], Loss: 1.8546\n",
      "Epoch [6/10], Step [11000], Loss: 1.8619\n",
      "Epoch [6/10], Step [11100], Loss: 1.8301\n",
      "Epoch [6/10], Step [11200], Loss: 1.8639\n",
      "Epoch [6/10], Step [11300], Loss: 1.8669\n",
      "Epoch [6/10], Step [11400], Loss: 1.8525\n",
      "Epoch [6/10], Step [11500], Loss: 1.8702\n",
      "Epoch [6/10], Step [11600], Loss: 1.8162\n",
      "Epoch [6/10], Step [11700], Loss: 1.7715\n",
      "Epoch [6/10], Step [11800], Loss: 1.8446\n",
      "Epoch [6/10], Step [11900], Loss: 1.8000\n",
      "Epoch [6/10], Step [12000], Loss: 1.8528\n",
      "Epoch [6/10], Step [12100], Loss: 1.8367\n",
      "Epoch [6/10], Step [12200], Loss: 1.8514\n",
      "Epoch [6/10], Step [12300], Loss: 1.8475\n",
      "Epoch [6/10], Step [12400], Loss: 1.8107\n",
      "Epoch [6/10], Step [12500], Loss: 1.8390\n",
      "Epoch [7/10], Step [100], Loss: 1.8030\n",
      "Epoch [7/10], Step [200], Loss: 1.8315\n",
      "Epoch [7/10], Step [300], Loss: 1.7935\n",
      "Epoch [7/10], Step [400], Loss: 1.8557\n",
      "Epoch [7/10], Step [500], Loss: 1.8391\n",
      "Epoch [7/10], Step [600], Loss: 1.8091\n",
      "Epoch [7/10], Step [700], Loss: 1.7881\n",
      "Epoch [7/10], Step [800], Loss: 1.8218\n",
      "Epoch [7/10], Step [900], Loss: 1.8580\n",
      "Epoch [7/10], Step [1000], Loss: 1.8240\n",
      "Epoch [7/10], Step [1100], Loss: 1.7909\n",
      "Epoch [7/10], Step [1200], Loss: 1.8265\n",
      "Epoch [7/10], Step [1300], Loss: 1.8121\n",
      "Epoch [7/10], Step [1400], Loss: 1.8189\n",
      "Epoch [7/10], Step [1500], Loss: 1.7868\n",
      "Epoch [7/10], Step [1600], Loss: 1.8413\n",
      "Epoch [7/10], Step [1700], Loss: 1.8141\n",
      "Epoch [7/10], Step [1800], Loss: 1.8556\n",
      "Epoch [7/10], Step [1900], Loss: 1.8210\n",
      "Epoch [7/10], Step [2000], Loss: 1.8509\n",
      "Epoch [7/10], Step [2100], Loss: 1.7928\n",
      "Epoch [7/10], Step [2200], Loss: 1.8062\n",
      "Epoch [7/10], Step [2300], Loss: 1.8249\n",
      "Epoch [7/10], Step [2400], Loss: 1.8335\n",
      "Epoch [7/10], Step [2500], Loss: 1.7855\n",
      "Epoch [7/10], Step [2600], Loss: 1.8303\n",
      "Epoch [7/10], Step [2700], Loss: 1.8119\n",
      "Epoch [7/10], Step [2800], Loss: 1.8012\n",
      "Epoch [7/10], Step [2900], Loss: 1.8348\n",
      "Epoch [7/10], Step [3000], Loss: 1.8169\n",
      "Epoch [7/10], Step [3100], Loss: 1.8359\n",
      "Epoch [7/10], Step [3200], Loss: 1.7974\n",
      "Epoch [7/10], Step [3300], Loss: 1.8287\n",
      "Epoch [7/10], Step [3400], Loss: 1.8332\n",
      "Epoch [7/10], Step [3500], Loss: 1.8184\n",
      "Epoch [7/10], Step [3600], Loss: 1.8413\n",
      "Epoch [7/10], Step [3700], Loss: 1.8798\n",
      "Epoch [7/10], Step [3800], Loss: 1.8040\n",
      "Epoch [7/10], Step [3900], Loss: 1.8155\n",
      "Epoch [7/10], Step [4000], Loss: 1.8199\n",
      "Epoch [7/10], Step [4100], Loss: 1.8292\n",
      "Epoch [7/10], Step [4200], Loss: 1.7922\n",
      "Epoch [7/10], Step [4300], Loss: 1.8434\n",
      "Epoch [7/10], Step [4400], Loss: 1.8183\n",
      "Epoch [7/10], Step [4500], Loss: 1.7966\n",
      "Epoch [7/10], Step [4600], Loss: 1.8260\n",
      "Epoch [7/10], Step [4700], Loss: 1.7924\n",
      "Epoch [7/10], Step [4800], Loss: 1.8115\n",
      "Epoch [7/10], Step [4900], Loss: 1.8290\n",
      "Epoch [7/10], Step [5000], Loss: 1.7995\n",
      "Epoch [7/10], Step [5100], Loss: 1.8261\n",
      "Epoch [7/10], Step [5200], Loss: 1.8292\n",
      "Epoch [7/10], Step [5300], Loss: 1.8315\n",
      "Epoch [7/10], Step [5400], Loss: 1.8235\n",
      "Epoch [7/10], Step [5500], Loss: 1.8190\n",
      "Epoch [7/10], Step [5600], Loss: 1.8569\n",
      "Epoch [7/10], Step [5700], Loss: 1.8571\n",
      "Epoch [7/10], Step [5800], Loss: 1.8310\n",
      "Epoch [7/10], Step [5900], Loss: 1.8281\n",
      "Epoch [7/10], Step [6000], Loss: 1.7946\n",
      "Epoch [7/10], Step [6100], Loss: 1.7935\n",
      "Epoch [7/10], Step [6200], Loss: 1.8676\n",
      "Epoch [7/10], Step [6300], Loss: 1.8435\n",
      "Epoch [7/10], Step [6400], Loss: 1.8449\n",
      "Epoch [7/10], Step [6500], Loss: 1.7947\n",
      "Epoch [7/10], Step [6600], Loss: 1.8466\n",
      "Epoch [7/10], Step [6700], Loss: 1.8095\n",
      "Epoch [7/10], Step [6800], Loss: 1.8066\n",
      "Epoch [7/10], Step [6900], Loss: 1.8671\n",
      "Epoch [7/10], Step [7000], Loss: 1.8244\n",
      "Epoch [7/10], Step [7100], Loss: 1.8239\n",
      "Epoch [7/10], Step [7200], Loss: 1.8051\n",
      "Epoch [7/10], Step [7300], Loss: 1.8027\n",
      "Epoch [7/10], Step [7400], Loss: 1.7775\n",
      "Epoch [7/10], Step [7500], Loss: 1.7911\n",
      "Epoch [7/10], Step [7600], Loss: 1.8164\n",
      "Epoch [7/10], Step [7700], Loss: 1.8015\n",
      "Epoch [7/10], Step [7800], Loss: 1.8372\n",
      "Epoch [7/10], Step [7900], Loss: 1.8509\n",
      "Epoch [7/10], Step [8000], Loss: 1.8198\n",
      "Epoch [7/10], Step [8100], Loss: 1.8226\n",
      "Epoch [7/10], Step [8200], Loss: 1.8403\n",
      "Epoch [7/10], Step [8300], Loss: 1.8115\n",
      "Epoch [7/10], Step [8400], Loss: 1.7893\n",
      "Epoch [7/10], Step [8500], Loss: 1.8280\n",
      "Epoch [7/10], Step [8600], Loss: 1.8087\n",
      "Epoch [7/10], Step [8700], Loss: 1.8087\n",
      "Epoch [7/10], Step [8800], Loss: 1.8572\n",
      "Epoch [7/10], Step [8900], Loss: 1.8443\n",
      "Epoch [7/10], Step [9000], Loss: 1.8380\n",
      "Epoch [7/10], Step [9100], Loss: 1.8225\n",
      "Epoch [7/10], Step [9200], Loss: 1.8181\n",
      "Epoch [7/10], Step [9300], Loss: 1.8286\n",
      "Epoch [7/10], Step [9400], Loss: 1.8016\n",
      "Epoch [7/10], Step [9500], Loss: 1.8395\n",
      "Epoch [7/10], Step [9600], Loss: 1.8368\n",
      "Epoch [7/10], Step [9700], Loss: 1.8190\n",
      "Epoch [7/10], Step [9800], Loss: 1.8031\n",
      "Epoch [7/10], Step [9900], Loss: 1.8229\n",
      "Epoch [7/10], Step [10000], Loss: 1.8268\n",
      "Epoch [7/10], Step [10100], Loss: 1.7749\n",
      "Epoch [7/10], Step [10200], Loss: 1.8472\n",
      "Epoch [7/10], Step [10300], Loss: 1.8110\n",
      "Epoch [7/10], Step [10400], Loss: 1.8414\n",
      "Epoch [7/10], Step [10500], Loss: 1.7638\n",
      "Epoch [7/10], Step [10600], Loss: 1.7892\n",
      "Epoch [7/10], Step [10700], Loss: 1.8366\n",
      "Epoch [7/10], Step [10800], Loss: 1.8150\n",
      "Epoch [7/10], Step [10900], Loss: 1.8078\n",
      "Epoch [7/10], Step [11000], Loss: 1.7945\n",
      "Epoch [7/10], Step [11100], Loss: 1.7861\n",
      "Epoch [7/10], Step [11200], Loss: 1.8034\n",
      "Epoch [7/10], Step [11300], Loss: 1.8051\n",
      "Epoch [7/10], Step [11400], Loss: 1.8183\n",
      "Epoch [7/10], Step [11500], Loss: 1.8177\n",
      "Epoch [7/10], Step [11600], Loss: 1.8393\n",
      "Epoch [7/10], Step [11700], Loss: 1.7832\n",
      "Epoch [7/10], Step [11800], Loss: 1.7975\n",
      "Epoch [7/10], Step [11900], Loss: 1.8471\n",
      "Epoch [7/10], Step [12000], Loss: 1.8058\n",
      "Epoch [7/10], Step [12100], Loss: 1.7653\n",
      "Epoch [7/10], Step [12200], Loss: 1.8136\n",
      "Epoch [7/10], Step [12300], Loss: 1.8242\n",
      "Epoch [7/10], Step [12400], Loss: 1.7964\n",
      "Epoch [7/10], Step [12500], Loss: 1.8324\n",
      "Epoch [8/10], Step [100], Loss: 1.8304\n",
      "Epoch [8/10], Step [200], Loss: 1.8248\n",
      "Epoch [8/10], Step [300], Loss: 1.7977\n",
      "Epoch [8/10], Step [400], Loss: 1.7685\n",
      "Epoch [8/10], Step [500], Loss: 1.7765\n",
      "Epoch [8/10], Step [600], Loss: 1.7948\n",
      "Epoch [8/10], Step [700], Loss: 1.7933\n",
      "Epoch [8/10], Step [800], Loss: 1.8175\n",
      "Epoch [8/10], Step [900], Loss: 1.7641\n",
      "Epoch [8/10], Step [1000], Loss: 1.7752\n",
      "Epoch [8/10], Step [1100], Loss: 1.8563\n",
      "Epoch [8/10], Step [1200], Loss: 1.7550\n",
      "Epoch [8/10], Step [1300], Loss: 1.8049\n",
      "Epoch [8/10], Step [1400], Loss: 1.8145\n",
      "Epoch [8/10], Step [1500], Loss: 1.7813\n",
      "Epoch [8/10], Step [1600], Loss: 1.8104\n",
      "Epoch [8/10], Step [1700], Loss: 1.7926\n",
      "Epoch [8/10], Step [1800], Loss: 1.7768\n",
      "Epoch [8/10], Step [1900], Loss: 1.8031\n",
      "Epoch [8/10], Step [2000], Loss: 1.8077\n",
      "Epoch [8/10], Step [2100], Loss: 1.7999\n",
      "Epoch [8/10], Step [2200], Loss: 1.7908\n",
      "Epoch [8/10], Step [2300], Loss: 1.7833\n",
      "Epoch [8/10], Step [2400], Loss: 1.7685\n",
      "Epoch [8/10], Step [2500], Loss: 1.7721\n",
      "Epoch [8/10], Step [2600], Loss: 1.8030\n",
      "Epoch [8/10], Step [2700], Loss: 1.8309\n",
      "Epoch [8/10], Step [2800], Loss: 1.7938\n",
      "Epoch [8/10], Step [2900], Loss: 1.7829\n",
      "Epoch [8/10], Step [3000], Loss: 1.7589\n",
      "Epoch [8/10], Step [3100], Loss: 1.7621\n",
      "Epoch [8/10], Step [3200], Loss: 1.7692\n",
      "Epoch [8/10], Step [3300], Loss: 1.7909\n",
      "Epoch [8/10], Step [3400], Loss: 1.7994\n",
      "Epoch [8/10], Step [3500], Loss: 1.7804\n",
      "Epoch [8/10], Step [3600], Loss: 1.8079\n",
      "Epoch [8/10], Step [3700], Loss: 1.7904\n",
      "Epoch [8/10], Step [3800], Loss: 1.7671\n",
      "Epoch [8/10], Step [3900], Loss: 1.8097\n",
      "Epoch [8/10], Step [4000], Loss: 1.8043\n",
      "Epoch [8/10], Step [4100], Loss: 1.7623\n",
      "Epoch [8/10], Step [4200], Loss: 1.8036\n",
      "Epoch [8/10], Step [4300], Loss: 1.7969\n",
      "Epoch [8/10], Step [4400], Loss: 1.7897\n",
      "Epoch [8/10], Step [4500], Loss: 1.8271\n",
      "Epoch [8/10], Step [4600], Loss: 1.8224\n",
      "Epoch [8/10], Step [4700], Loss: 1.8151\n",
      "Epoch [8/10], Step [4800], Loss: 1.7797\n",
      "Epoch [8/10], Step [4900], Loss: 1.8153\n",
      "Epoch [8/10], Step [5000], Loss: 1.7838\n",
      "Epoch [8/10], Step [5100], Loss: 1.8253\n",
      "Epoch [8/10], Step [5200], Loss: 1.7592\n",
      "Epoch [8/10], Step [5300], Loss: 1.7841\n",
      "Epoch [8/10], Step [5400], Loss: 1.7862\n",
      "Epoch [8/10], Step [5500], Loss: 1.7662\n",
      "Epoch [8/10], Step [5600], Loss: 1.7834\n",
      "Epoch [8/10], Step [5700], Loss: 1.8710\n",
      "Epoch [8/10], Step [5800], Loss: 1.8204\n",
      "Epoch [8/10], Step [5900], Loss: 1.7851\n",
      "Epoch [8/10], Step [6000], Loss: 1.7863\n",
      "Epoch [8/10], Step [6100], Loss: 1.8005\n",
      "Epoch [8/10], Step [6200], Loss: 1.7751\n",
      "Epoch [8/10], Step [6300], Loss: 1.8185\n",
      "Epoch [8/10], Step [6400], Loss: 1.7692\n",
      "Epoch [8/10], Step [6500], Loss: 1.8061\n",
      "Epoch [8/10], Step [6600], Loss: 1.8052\n",
      "Epoch [8/10], Step [6700], Loss: 1.7830\n",
      "Epoch [8/10], Step [6800], Loss: 1.7999\n",
      "Epoch [8/10], Step [6900], Loss: 1.7478\n",
      "Epoch [8/10], Step [7000], Loss: 1.7858\n",
      "Epoch [8/10], Step [7100], Loss: 1.7771\n",
      "Epoch [8/10], Step [7200], Loss: 1.7649\n",
      "Epoch [8/10], Step [7300], Loss: 1.7940\n",
      "Epoch [8/10], Step [7400], Loss: 1.7936\n",
      "Epoch [8/10], Step [7500], Loss: 1.8175\n",
      "Epoch [8/10], Step [7600], Loss: 1.8101\n",
      "Epoch [8/10], Step [7700], Loss: 1.8121\n",
      "Epoch [8/10], Step [7800], Loss: 1.8074\n",
      "Epoch [8/10], Step [7900], Loss: 1.7488\n",
      "Epoch [8/10], Step [8000], Loss: 1.7872\n",
      "Epoch [8/10], Step [8100], Loss: 1.8267\n",
      "Epoch [8/10], Step [8200], Loss: 1.8210\n",
      "Epoch [8/10], Step [8300], Loss: 1.7985\n",
      "Epoch [8/10], Step [8400], Loss: 1.7721\n",
      "Epoch [8/10], Step [8500], Loss: 1.7822\n",
      "Epoch [8/10], Step [8600], Loss: 1.7913\n",
      "Epoch [8/10], Step [8700], Loss: 1.7768\n",
      "Epoch [8/10], Step [8800], Loss: 1.7852\n",
      "Epoch [8/10], Step [8900], Loss: 1.7843\n",
      "Epoch [8/10], Step [9000], Loss: 1.8056\n",
      "Epoch [8/10], Step [9100], Loss: 1.7483\n",
      "Epoch [8/10], Step [9200], Loss: 1.7581\n",
      "Epoch [8/10], Step [9300], Loss: 1.8114\n",
      "Epoch [8/10], Step [9400], Loss: 1.7635\n",
      "Epoch [8/10], Step [9500], Loss: 1.7562\n",
      "Epoch [8/10], Step [9600], Loss: 1.7737\n",
      "Epoch [8/10], Step [9700], Loss: 1.8459\n",
      "Epoch [8/10], Step [9800], Loss: 1.8057\n",
      "Epoch [8/10], Step [9900], Loss: 1.7691\n",
      "Epoch [8/10], Step [10000], Loss: 1.7952\n",
      "Epoch [8/10], Step [10100], Loss: 1.8126\n",
      "Epoch [8/10], Step [10200], Loss: 1.8381\n",
      "Epoch [8/10], Step [10300], Loss: 1.7893\n",
      "Epoch [8/10], Step [10400], Loss: 1.8049\n",
      "Epoch [8/10], Step [10500], Loss: 1.7537\n",
      "Epoch [8/10], Step [10600], Loss: 1.7938\n",
      "Epoch [8/10], Step [10700], Loss: 1.8265\n",
      "Epoch [8/10], Step [10800], Loss: 1.7938\n",
      "Epoch [8/10], Step [10900], Loss: 1.7701\n",
      "Epoch [8/10], Step [11000], Loss: 1.7747\n",
      "Epoch [8/10], Step [11100], Loss: 1.7608\n",
      "Epoch [8/10], Step [11200], Loss: 1.7502\n",
      "Epoch [8/10], Step [11300], Loss: 1.7864\n",
      "Epoch [8/10], Step [11400], Loss: 1.7696\n",
      "Epoch [8/10], Step [11500], Loss: 1.8084\n",
      "Epoch [8/10], Step [11600], Loss: 1.7612\n",
      "Epoch [8/10], Step [11700], Loss: 1.7781\n",
      "Epoch [8/10], Step [11800], Loss: 1.7764\n",
      "Epoch [8/10], Step [11900], Loss: 1.7698\n",
      "Epoch [8/10], Step [12000], Loss: 1.8063\n",
      "Epoch [8/10], Step [12100], Loss: 1.8315\n",
      "Epoch [8/10], Step [12200], Loss: 1.7629\n",
      "Epoch [8/10], Step [12300], Loss: 1.7839\n",
      "Epoch [8/10], Step [12400], Loss: 1.8027\n",
      "Epoch [8/10], Step [12500], Loss: 1.7846\n",
      "Epoch [9/10], Step [100], Loss: 1.7690\n",
      "Epoch [9/10], Step [200], Loss: 1.7780\n",
      "Epoch [9/10], Step [300], Loss: 1.7408\n",
      "Epoch [9/10], Step [400], Loss: 1.7470\n",
      "Epoch [9/10], Step [500], Loss: 1.7921\n",
      "Epoch [9/10], Step [600], Loss: 1.7722\n",
      "Epoch [9/10], Step [700], Loss: 1.7699\n",
      "Epoch [9/10], Step [800], Loss: 1.7397\n",
      "Epoch [9/10], Step [900], Loss: 1.7350\n",
      "Epoch [9/10], Step [1000], Loss: 1.7505\n",
      "Epoch [9/10], Step [1100], Loss: 1.7383\n",
      "Epoch [9/10], Step [1200], Loss: 1.7693\n",
      "Epoch [9/10], Step [1300], Loss: 1.7644\n",
      "Epoch [9/10], Step [1400], Loss: 1.7516\n",
      "Epoch [9/10], Step [1500], Loss: 1.7444\n",
      "Epoch [9/10], Step [1600], Loss: 1.7520\n",
      "Epoch [9/10], Step [1700], Loss: 1.7562\n",
      "Epoch [9/10], Step [1800], Loss: 1.7884\n",
      "Epoch [9/10], Step [1900], Loss: 1.7549\n",
      "Epoch [9/10], Step [2000], Loss: 1.7692\n",
      "Epoch [9/10], Step [2100], Loss: 1.7676\n",
      "Epoch [9/10], Step [2200], Loss: 1.7799\n",
      "Epoch [9/10], Step [2300], Loss: 1.7571\n",
      "Epoch [9/10], Step [2400], Loss: 1.7410\n",
      "Epoch [9/10], Step [2500], Loss: 1.7455\n",
      "Epoch [9/10], Step [2600], Loss: 1.7446\n",
      "Epoch [9/10], Step [2700], Loss: 1.7788\n",
      "Epoch [9/10], Step [2800], Loss: 1.7796\n",
      "Epoch [9/10], Step [2900], Loss: 1.7262\n",
      "Epoch [9/10], Step [3000], Loss: 1.7404\n",
      "Epoch [9/10], Step [3100], Loss: 1.7359\n",
      "Epoch [9/10], Step [3200], Loss: 1.7519\n",
      "Epoch [9/10], Step [3300], Loss: 1.7357\n",
      "Epoch [9/10], Step [3400], Loss: 1.7850\n",
      "Epoch [9/10], Step [3500], Loss: 1.7503\n",
      "Epoch [9/10], Step [3600], Loss: 1.7608\n",
      "Epoch [9/10], Step [3700], Loss: 1.7770\n",
      "Epoch [9/10], Step [3800], Loss: 1.7559\n",
      "Epoch [9/10], Step [3900], Loss: 1.7960\n",
      "Epoch [9/10], Step [4000], Loss: 1.7465\n",
      "Epoch [9/10], Step [4100], Loss: 1.7949\n",
      "Epoch [9/10], Step [4200], Loss: 1.7438\n",
      "Epoch [9/10], Step [4300], Loss: 1.7374\n",
      "Epoch [9/10], Step [4400], Loss: 1.7629\n",
      "Epoch [9/10], Step [4500], Loss: 1.7697\n",
      "Epoch [9/10], Step [4600], Loss: 1.7490\n",
      "Epoch [9/10], Step [4700], Loss: 1.8064\n",
      "Epoch [9/10], Step [4800], Loss: 1.8188\n",
      "Epoch [9/10], Step [4900], Loss: 1.7655\n",
      "Epoch [9/10], Step [5000], Loss: 1.8114\n",
      "Epoch [9/10], Step [5100], Loss: 1.7368\n",
      "Epoch [9/10], Step [5200], Loss: 1.7412\n",
      "Epoch [9/10], Step [5300], Loss: 1.8096\n",
      "Epoch [9/10], Step [5400], Loss: 1.7819\n",
      "Epoch [9/10], Step [5500], Loss: 1.7775\n",
      "Epoch [9/10], Step [5600], Loss: 1.7754\n",
      "Epoch [9/10], Step [5700], Loss: 1.7876\n",
      "Epoch [9/10], Step [5800], Loss: 1.7747\n",
      "Epoch [9/10], Step [5900], Loss: 1.7982\n",
      "Epoch [9/10], Step [6000], Loss: 1.8069\n",
      "Epoch [9/10], Step [6100], Loss: 1.7521\n",
      "Epoch [9/10], Step [6200], Loss: 1.7770\n",
      "Epoch [9/10], Step [6300], Loss: 1.7744\n",
      "Epoch [9/10], Step [6400], Loss: 1.7737\n",
      "Epoch [9/10], Step [6500], Loss: 1.7414\n",
      "Epoch [9/10], Step [6600], Loss: 1.7705\n",
      "Epoch [9/10], Step [6700], Loss: 1.7579\n",
      "Epoch [9/10], Step [6800], Loss: 1.7831\n",
      "Epoch [9/10], Step [6900], Loss: 1.7480\n",
      "Epoch [9/10], Step [7000], Loss: 1.7715\n",
      "Epoch [9/10], Step [7100], Loss: 1.7721\n",
      "Epoch [9/10], Step [7200], Loss: 1.7971\n",
      "Epoch [9/10], Step [7300], Loss: 1.7699\n",
      "Epoch [9/10], Step [7400], Loss: 1.7639\n",
      "Epoch [9/10], Step [7500], Loss: 1.7228\n",
      "Epoch [9/10], Step [7600], Loss: 1.7527\n",
      "Epoch [9/10], Step [7700], Loss: 1.7585\n",
      "Epoch [9/10], Step [7800], Loss: 1.7792\n",
      "Epoch [9/10], Step [7900], Loss: 1.7589\n",
      "Epoch [9/10], Step [8000], Loss: 1.7791\n",
      "Epoch [9/10], Step [8100], Loss: 1.7623\n",
      "Epoch [9/10], Step [8200], Loss: 1.7943\n",
      "Epoch [9/10], Step [8300], Loss: 1.7725\n",
      "Epoch [9/10], Step [8400], Loss: 1.7499\n",
      "Epoch [9/10], Step [8500], Loss: 1.7793\n",
      "Epoch [9/10], Step [8600], Loss: 1.7865\n",
      "Epoch [9/10], Step [8700], Loss: 1.7917\n",
      "Epoch [9/10], Step [8800], Loss: 1.7534\n",
      "Epoch [9/10], Step [8900], Loss: 1.7366\n",
      "Epoch [9/10], Step [9000], Loss: 1.7489\n",
      "Epoch [9/10], Step [9100], Loss: 1.8132\n",
      "Epoch [9/10], Step [9200], Loss: 1.7800\n",
      "Epoch [9/10], Step [9300], Loss: 1.7986\n",
      "Epoch [9/10], Step [9400], Loss: 1.7368\n",
      "Epoch [9/10], Step [9500], Loss: 1.7955\n",
      "Epoch [9/10], Step [9600], Loss: 1.7770\n",
      "Epoch [9/10], Step [9700], Loss: 1.7909\n",
      "Epoch [9/10], Step [9800], Loss: 1.7762\n",
      "Epoch [9/10], Step [9900], Loss: 1.7750\n",
      "Epoch [9/10], Step [10000], Loss: 1.8214\n",
      "Epoch [9/10], Step [10100], Loss: 1.7703\n",
      "Epoch [9/10], Step [10200], Loss: 1.7886\n",
      "Epoch [9/10], Step [10300], Loss: 1.7882\n",
      "Epoch [9/10], Step [10400], Loss: 1.8004\n",
      "Epoch [9/10], Step [10500], Loss: 1.7139\n",
      "Epoch [9/10], Step [10600], Loss: 1.7725\n",
      "Epoch [9/10], Step [10700], Loss: 1.7958\n",
      "Epoch [9/10], Step [10800], Loss: 1.7318\n",
      "Epoch [9/10], Step [10900], Loss: 1.7712\n",
      "Epoch [9/10], Step [11000], Loss: 1.7731\n",
      "Epoch [9/10], Step [11100], Loss: 1.7862\n",
      "Epoch [9/10], Step [11200], Loss: 1.7656\n",
      "Epoch [9/10], Step [11300], Loss: 1.7634\n",
      "Epoch [9/10], Step [11400], Loss: 1.7588\n",
      "Epoch [9/10], Step [11500], Loss: 1.7546\n",
      "Epoch [9/10], Step [11600], Loss: 1.7516\n",
      "Epoch [9/10], Step [11700], Loss: 1.7311\n",
      "Epoch [9/10], Step [11800], Loss: 1.7828\n",
      "Epoch [9/10], Step [11900], Loss: 1.7490\n",
      "Epoch [9/10], Step [12000], Loss: 1.7413\n",
      "Epoch [9/10], Step [12100], Loss: 1.7700\n",
      "Epoch [9/10], Step [12200], Loss: 1.7634\n",
      "Epoch [9/10], Step [12300], Loss: 1.7690\n",
      "Epoch [9/10], Step [12400], Loss: 1.7709\n",
      "Epoch [9/10], Step [12500], Loss: 1.7557\n",
      "Epoch [10/10], Step [100], Loss: 1.7483\n",
      "Epoch [10/10], Step [200], Loss: 1.7386\n",
      "Epoch [10/10], Step [300], Loss: 1.7308\n",
      "Epoch [10/10], Step [400], Loss: 1.7415\n",
      "Epoch [10/10], Step [500], Loss: 1.7288\n",
      "Epoch [10/10], Step [600], Loss: 1.7441\n",
      "Epoch [10/10], Step [700], Loss: 1.7913\n",
      "Epoch [10/10], Step [800], Loss: 1.7467\n",
      "Epoch [10/10], Step [900], Loss: 1.7524\n",
      "Epoch [10/10], Step [1000], Loss: 1.7338\n",
      "Epoch [10/10], Step [1100], Loss: 1.7284\n",
      "Epoch [10/10], Step [1200], Loss: 1.7883\n",
      "Epoch [10/10], Step [1300], Loss: 1.7487\n",
      "Epoch [10/10], Step [1400], Loss: 1.7561\n",
      "Epoch [10/10], Step [1500], Loss: 1.7338\n",
      "Epoch [10/10], Step [1600], Loss: 1.7270\n",
      "Epoch [10/10], Step [1700], Loss: 1.7412\n",
      "Epoch [10/10], Step [1800], Loss: 1.7552\n",
      "Epoch [10/10], Step [1900], Loss: 1.7275\n",
      "Epoch [10/10], Step [2000], Loss: 1.7324\n",
      "Epoch [10/10], Step [2100], Loss: 1.7278\n",
      "Epoch [10/10], Step [2200], Loss: 1.7087\n",
      "Epoch [10/10], Step [2300], Loss: 1.7401\n",
      "Epoch [10/10], Step [2400], Loss: 1.7502\n",
      "Epoch [10/10], Step [2500], Loss: 1.7523\n",
      "Epoch [10/10], Step [2600], Loss: 1.7198\n",
      "Epoch [10/10], Step [2700], Loss: 1.7294\n",
      "Epoch [10/10], Step [2800], Loss: 1.7241\n",
      "Epoch [10/10], Step [2900], Loss: 1.7284\n",
      "Epoch [10/10], Step [3000], Loss: 1.7248\n",
      "Epoch [10/10], Step [3100], Loss: 1.7595\n",
      "Epoch [10/10], Step [3200], Loss: 1.7600\n",
      "Epoch [10/10], Step [3300], Loss: 1.6963\n",
      "Epoch [10/10], Step [3400], Loss: 1.7705\n",
      "Epoch [10/10], Step [3500], Loss: 1.7541\n",
      "Epoch [10/10], Step [3600], Loss: 1.7702\n",
      "Epoch [10/10], Step [3700], Loss: 1.7536\n",
      "Epoch [10/10], Step [3800], Loss: 1.7333\n",
      "Epoch [10/10], Step [3900], Loss: 1.7558\n",
      "Epoch [10/10], Step [4000], Loss: 1.7191\n",
      "Epoch [10/10], Step [4100], Loss: 1.7265\n",
      "Epoch [10/10], Step [4200], Loss: 1.7500\n",
      "Epoch [10/10], Step [4300], Loss: 1.7335\n",
      "Epoch [10/10], Step [4400], Loss: 1.7456\n",
      "Epoch [10/10], Step [4500], Loss: 1.7268\n",
      "Epoch [10/10], Step [4600], Loss: 1.7259\n",
      "Epoch [10/10], Step [4700], Loss: 1.7805\n",
      "Epoch [10/10], Step [4800], Loss: 1.7189\n",
      "Epoch [10/10], Step [4900], Loss: 1.7633\n",
      "Epoch [10/10], Step [5000], Loss: 1.6911\n",
      "Epoch [10/10], Step [5100], Loss: 1.7644\n",
      "Epoch [10/10], Step [5200], Loss: 1.7364\n",
      "Epoch [10/10], Step [5300], Loss: 1.7527\n",
      "Epoch [10/10], Step [5400], Loss: 1.7188\n",
      "Epoch [10/10], Step [5500], Loss: 1.7577\n",
      "Epoch [10/10], Step [5600], Loss: 1.7229\n",
      "Epoch [10/10], Step [5700], Loss: 1.7613\n",
      "Epoch [10/10], Step [5800], Loss: 1.7770\n",
      "Epoch [10/10], Step [5900], Loss: 1.7118\n",
      "Epoch [10/10], Step [6000], Loss: 1.7792\n",
      "Epoch [10/10], Step [6100], Loss: 1.6876\n",
      "Epoch [10/10], Step [6200], Loss: 1.7330\n",
      "Epoch [10/10], Step [6300], Loss: 1.7293\n",
      "Epoch [10/10], Step [6400], Loss: 1.7325\n",
      "Epoch [10/10], Step [6500], Loss: 1.7567\n",
      "Epoch [10/10], Step [6600], Loss: 1.7836\n",
      "Epoch [10/10], Step [6700], Loss: 1.7434\n",
      "Epoch [10/10], Step [6800], Loss: 1.7378\n",
      "Epoch [10/10], Step [6900], Loss: 1.7708\n",
      "Epoch [10/10], Step [7000], Loss: 1.7472\n",
      "Epoch [10/10], Step [7100], Loss: 1.7140\n",
      "Epoch [10/10], Step [7200], Loss: 1.7879\n",
      "Epoch [10/10], Step [7300], Loss: 1.7184\n",
      "Epoch [10/10], Step [7400], Loss: 1.7175\n",
      "Epoch [10/10], Step [7500], Loss: 1.7522\n",
      "Epoch [10/10], Step [7600], Loss: 1.7461\n",
      "Epoch [10/10], Step [7700], Loss: 1.7532\n",
      "Epoch [10/10], Step [7800], Loss: 1.7259\n",
      "Epoch [10/10], Step [7900], Loss: 1.7338\n",
      "Epoch [10/10], Step [8000], Loss: 1.7515\n",
      "Epoch [10/10], Step [8100], Loss: 1.7468\n",
      "Epoch [10/10], Step [8200], Loss: 1.7471\n",
      "Epoch [10/10], Step [8300], Loss: 1.7591\n",
      "Epoch [10/10], Step [8400], Loss: 1.7232\n",
      "Epoch [10/10], Step [8500], Loss: 1.7297\n",
      "Epoch [10/10], Step [8600], Loss: 1.7517\n",
      "Epoch [10/10], Step [8700], Loss: 1.7477\n",
      "Epoch [10/10], Step [8800], Loss: 1.7155\n",
      "Epoch [10/10], Step [8900], Loss: 1.7849\n",
      "Epoch [10/10], Step [9000], Loss: 1.7762\n",
      "Epoch [10/10], Step [9100], Loss: 1.7563\n",
      "Epoch [10/10], Step [9200], Loss: 1.7304\n",
      "Epoch [10/10], Step [9300], Loss: 1.7274\n",
      "Epoch [10/10], Step [9400], Loss: 1.7739\n",
      "Epoch [10/10], Step [9500], Loss: 1.7087\n",
      "Epoch [10/10], Step [9600], Loss: 1.7199\n",
      "Epoch [10/10], Step [9700], Loss: 1.7454\n",
      "Epoch [10/10], Step [9800], Loss: 1.7242\n",
      "Epoch [10/10], Step [9900], Loss: 1.7694\n",
      "Epoch [10/10], Step [10000], Loss: 1.6835\n",
      "Epoch [10/10], Step [10100], Loss: 1.7468\n",
      "Epoch [10/10], Step [10200], Loss: 1.7571\n",
      "Epoch [10/10], Step [10300], Loss: 1.7436\n",
      "Epoch [10/10], Step [10400], Loss: 1.7381\n",
      "Epoch [10/10], Step [10500], Loss: 1.7563\n",
      "Epoch [10/10], Step [10600], Loss: 1.7472\n",
      "Epoch [10/10], Step [10700], Loss: 1.7362\n",
      "Epoch [10/10], Step [10800], Loss: 1.7596\n",
      "Epoch [10/10], Step [10900], Loss: 1.7544\n",
      "Epoch [10/10], Step [11000], Loss: 1.7470\n",
      "Epoch [10/10], Step [11100], Loss: 1.7458\n",
      "Epoch [10/10], Step [11200], Loss: 1.7509\n",
      "Epoch [10/10], Step [11300], Loss: 1.7467\n",
      "Epoch [10/10], Step [11400], Loss: 1.7045\n",
      "Epoch [10/10], Step [11500], Loss: 1.7466\n",
      "Epoch [10/10], Step [11600], Loss: 1.7598\n",
      "Epoch [10/10], Step [11700], Loss: 1.7561\n",
      "Epoch [10/10], Step [11800], Loss: 1.7163\n",
      "Epoch [10/10], Step [11900], Loss: 1.7204\n",
      "Epoch [10/10], Step [12000], Loss: 1.7447\n",
      "Epoch [10/10], Step [12100], Loss: 1.7286\n",
      "Epoch [10/10], Step [12200], Loss: 1.7503\n",
      "Epoch [10/10], Step [12300], Loss: 1.7108\n",
      "Epoch [10/10], Step [12400], Loss: 1.7636\n",
      "Epoch [10/10], Step [12500], Loss: 1.7643\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99: \n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0984b68f-5449-41b9-adea-4e34f5dbe21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b405b45-7231-4121-9a10-cb0ad5a06806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 66.64%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images,labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f01ca1d6-22bb-4ceb-a41d-bb70b22806d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c45612d-bf23-49ee-a974-ed324bef49cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: dog\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "def load_image(image_path):\n",
    "    if image_path.startswith('http://') or image_path.startswith('https://'):\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize((32, 32)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def predict(image_path):\n",
    "    image = load_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        out = model(image)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "image_path = 'https://www.princeton.edu/sites/default/files/styles/half_16_9_1440/public/images/2022/02/KOA_Nassau_2697x1517.jpg?itok=Nd5wwOFu'\n",
    "predicted_class = predict(image_path)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "print(f'clase predicha: {classes[predicted_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9831c-2ef9-4868-8648-b0d121e41a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd08cd88-bd98-4758-ad42-d7a8c3f70670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
